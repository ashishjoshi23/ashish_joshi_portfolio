
##Overview
A smart glove-based system that translates hand gestures into text and speech in real-time, designed to assist deaf and mute individuals in communicating with non-sign language users. The project integrates both word-level gesture recognition using Arduino and sensors, and fingerspelling recognition using a CNN-based deep learning model.

##Key Features
Word-Level Gesture Vocalizer: Uses flex sensors, MPU6050 accelerometer, and Arduino to detect gestures and output pre-defined words.

Fingerspelling ASL Recognition: CNN model trained to recognize American Sign Language (ASL) alphabets from webcam input.

Real-Time Text & Speech Output: Converts gestures to text and speech via Bluetooth and a mobile app.

Autocorrect Feature: Uses Hunspell Suggest to correct spelling errors in formed words.

Confusion Matrix Analysis: Model accuracy of 93%, precision 95.19%, recall 93.1%.

##Technologies Used
Hardware: Arduino UNO, MPU6050, HC-05 Bluetooth, Flex Sensors, Button Switch

Software: Arduino IDE, Python, TensorFlow, Keras, OpenCV, NumPy, Matplotlib

Libraries: Hunspell, scikit-learn, tkinter (for GUI)

##Results

Accuracy: 93%
Precision: 95.19%
Recall: 93.1%
F1 Score: 94.08%
